# ğŸ§  Smart PDF Chat Analyzer (Offline LLM)

Ein lokaler KI-gestÃ¼tzter PDF-Chatbot, der PDF-Inhalte analysiert und in natÃ¼rlicher Sprache beantwortet â€“ **vollstÃ¤ndig offline**, ohne Cloud, ohne API-Key.  
Entwickelt mit **Python**, **Streamlit**, **llama-cpp-python** und **PyMuPDF**.

---

## ğŸš€ ProjektÃ¼bersicht

Dieses Projekt ermÃ¶glicht es, ein PDF-Dokument (z.â€¯B. Vertrag, Report, E-Book) hochzuladen und in natÃ¼rlicher Sprache Fragen dazu zu stellen.  
Die Antworten kommen von einem **lokal laufenden LLM (Large Language Model)** wie z.â€¯B. TinyLlama, das auf dem Rechner des Users ausgefÃ¼hrt wird.

### ğŸ” Kein Cloud-Zwang:
- Keine Registrierung
- Kein API-Key (wie bei OpenAI)
- 100â€¯% lokal & datenschutzfreundlich

---

## ğŸ§± Technologien

| Funktion | Tool |
|----------|------|
| PDF-Text extrahieren | [`PyMuPDF`](https://pymupdf.readthedocs.io/) |
| Chat-Interface (Frontend) | [`Streamlit`](https://streamlit.io) |
| Offline LLM-Backend | [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) |
| Modellformat | GGUF / GGML (z.â€¯B. TinyLlama, Mistral, Vicuna) |

---

## ğŸ“‚ Projektstruktur

```plaintext
SmartPDFChat/
â”œâ”€â”€ app.py               # Streamlit App-Logik & UI
â”œâ”€â”€ pdf_reader.py        # Extrahiert Text aus PDF (mit PyMuPDF)
â”œâ”€â”€ chat_engine.py       # Bindet das lokale LLM Ã¼ber llama-cpp-python ein
â”œâ”€â”€ requirements.txt     # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ models/              # Lokale LLM-Modelle (.gguf/.ggml)
â””â”€â”€ README.md            # Diese Datei

### ğŸ“Œ Hinweis

Das Projekt befindet sich in aktiver Weiterentwicklung.  
Ein separates, benutzerfreundlicheres Frontend ist bereits in Planung, um die AI-FunktionalitÃ¤t optimal zu prÃ¤sentieren.
